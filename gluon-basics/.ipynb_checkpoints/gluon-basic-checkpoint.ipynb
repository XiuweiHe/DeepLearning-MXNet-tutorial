{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建网络\n",
    "解释gluon如何工作，之前使用的nn.Sequential，它是nn.Block的一个简单形式，我们并没有深入了解它们。\n",
    "\n",
    "本教程和接下来几个教程，我们将详细解释如何使用这两个类来定义神经网络、初始化参数、以及保存和读取模型。\n",
    "\n",
    "我们重新把多层感知机 — 使用Gluon里的网络定义搬到这里作为开始的例子（为了简单起见，这里我们丢掉了Flatten层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dense(None -> 256, Activation(relu))\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(256, activation= 'relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 `nn.Block` 定义\n",
    "`nn.Sequential`是`nn.Block`的简单形式，如下用`nn.Block`实现同样的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    \"\"\"\n",
    "    __init__:创建参数。上面例子我们使用了包含了参数的dense层\n",
    "    \"\"\"\n",
    "    def __init__(self, ** kwargs):\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__(**kwargs)函数：这句话调用nn.Block的__init__函数，\n",
    "        它提供了prefix（指定名字）和params（指定模型参数）两个参数。\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__(** kwargs)\n",
    "        \"\"\"\n",
    "        调用nn.Block提供的name_scope()函数。nn.Dense的定义放在这个scope里面。\n",
    "        它的作用是给里面的所有层和参数的名字加上前缀（prefix）使得他们在系统里面独一无二。\n",
    "        默认自动会自动生成前缀，我们也可以在创建的时候手动指定。推荐在构建网络时，每个层至少在一个name_scope()里\n",
    "        \"\"\"\n",
    "        with self.name_scope():\n",
    "            self.dense0 = nn.Dense(256)\n",
    "            self.dense1 = nn.Dense(10)\n",
    "    # 定义前向网络计算\n",
    "    def forward(self, x):\n",
    "        return self.dense1(nd.relu(self.dense0(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (dense1): Dense(None -> 10, linear)\n",
      "  (dense0): Dense(None -> 256, linear)\n",
      ")\n",
      "Parameter mlp3_dense0_weight (shape=(256, 0), dtype=<class 'numpy.float32'>)\n",
      "\n",
      "[[-0.00280519  0.05682168  0.00845101 -0.07698126 -0.06098332  0.05909928\n",
      "   0.03550563 -0.01684654 -0.02854338  0.08916292]\n",
      " [-0.00262184  0.04471714  0.04032315 -0.07430363 -0.02478844  0.03125754\n",
      "   0.01593504 -0.06641655  0.02483857  0.01304245]\n",
      " [ 0.0029594   0.0288551  -0.03639418 -0.11928524 -0.0427849   0.03652045\n",
      "   0.08650892 -0.01271457 -0.02150599  0.09514914]\n",
      " [ 0.03211017 -0.00701749 -0.05499509 -0.068024   -0.05132047  0.03421191\n",
      "   0.04185638  0.00668136 -0.02798838  0.06224423]]\n",
      "<NDArray 4x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net2 = MLP()\n",
    "print(net2)\n",
    "print(net2.dense0.weight)\n",
    "net2.initialize()\n",
    "x = nd.random_uniform(shape=(4,20))\n",
    "print(net2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default prefix: mlp3_dense0\n",
      "customized prefix: another_mlp_dense0\n"
     ]
    }
   ],
   "source": [
    "print('default prefix:', net2.dense0.name)\n",
    "\n",
    "net3 = MLP(prefix='another_mlp_')\n",
    "print('customized prefix:', net3.dense0.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Block到底是什么东西？\n",
    "在gluon里，nn.Block是一个一般化的部件。整个神经网络可以是一个nn.Block，单个层也是一个nn.Block。我们可以（近似）无限地嵌套nn.Block来构建新的nn.Block。\n",
    "\n",
    "nn.Block主要提供这个东西\n",
    "\n",
    "1. 存储参数\n",
    "2. 描述forward如何执行\n",
    "3. 自动求导\n",
    "## 那么现在可以解释nn.Sequential了吧\n",
    "nn.Sequential是一个nn.Block容器，它通过add来添加nn.Block。它自动生成forward()函数，其就是把加进来的nn.Block逐一运行。\n",
    "\n",
    "一个简单的实现是这样的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.00104139  0.00639622  0.03454763 -0.01168697  0.07835874 -0.02142669\n",
      "  -0.00592941 -0.05849914  0.04525637  0.00457406]\n",
      " [ 0.02241309 -0.04296242  0.03766069 -0.00279771  0.0500275  -0.03436268\n",
      "  -0.00224496 -0.02926439  0.04133548  0.0105134 ]\n",
      " [-0.03016665  0.01821524  0.04116601 -0.01005435  0.04715824 -0.01262928\n",
      "  -0.00871191 -0.05950979  0.00829933 -0.00059525]\n",
      " [ 0.0063363   0.00877344  0.0233789   0.0061211   0.03473819 -0.02619518\n",
      "  -0.00732469 -0.01876338  0.05770727 -0.00276955]]\n",
      "<NDArray 4x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "class Sequential(nn.Block):\n",
    "    def __init__(self, ** kwargs):\n",
    "        super(Sequential, self).__init__(** kwargs)\n",
    "    \n",
    "    def add(self,block):\n",
    "        self._children.append(block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self._children:\n",
    "            x = block(x)\n",
    "        return x\n",
    "# 可以和 nn.Sequential() 一样来使用自定义的类\n",
    "net4 = Sequential()\n",
    "with net4.name_scope():\n",
    "    net4.add(nn.Dense(256, activation= 'relu'))\n",
    "    net4.add(nn.Dense(10))\n",
    "net4.initialize()\n",
    "print(net4(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Block和nn.Sequential的嵌套使用\n",
    "现在我们知道了nn下面的类基本都是nn.Block的子类，他们可以很方便地嵌套使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): RecMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Dense(None -> 256, Activation(relu))\n",
      "      (1): Dense(None -> 128, Activation(relu))\n",
      "    )\n",
      "    (dense): Dense(None -> 64, linear)\n",
      "  )\n",
      "  (1): Dense(None -> 10, linear)\n",
      ")\n",
      "\n",
      "[[  3.62937967e-03  -3.64664244e-03  -3.40712070e-03  -3.43851116e-03\n",
      "   -3.74851632e-03   1.42319086e-05  -9.07513604e-04  -2.00890796e-03\n",
      "    2.85684969e-03  -4.77109279e-04]\n",
      " [  1.67747529e-03  -4.78566950e-03  -3.25752934e-03  -1.41125685e-03\n",
      "   -3.41535360e-03   1.90782256e-03   2.87884723e-05  -1.88271201e-03\n",
      "    1.25917548e-03  -5.82630892e-05]\n",
      " [  2.97360495e-03  -6.60494808e-03  -4.10671020e-03  -2.74736341e-03\n",
      "   -8.19559395e-03   1.49177841e-03  -1.15507864e-03  -3.66006698e-03\n",
      "    1.57480477e-03   1.19225704e-03]\n",
      " [  4.24453150e-03  -6.48297556e-03  -4.90886392e-03  -5.40651660e-03\n",
      "   -5.40666375e-03  -6.01428968e-04  -5.18403889e-04  -3.52386502e-03\n",
      "    2.24238355e-03  -2.10732245e-03]]\n",
      "<NDArray 4x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "class RecMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RecMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        with self.name_scope():\n",
    "            self.net.add(nn.Dense(256, activation=\"relu\"))\n",
    "            self.net.add(nn.Dense(128, activation=\"relu\"))\n",
    "            self.dense = nn.Dense(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nd.relu(self.dense(self.net(x)))\n",
    "\n",
    "rec_mlp = nn.Sequential()\n",
    "rec_mlp.add(RecMLP())\n",
    "rec_mlp.add(nn.Dense(10))\n",
    "rec_mlp.initialize()\n",
    "print(rec_mlp)\n",
    "print(rec_mlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecMLP1(\n",
      "  (dense1): Dense(None -> 128, Activation(relu))\n",
      "  (dense0): Dense(None -> 256, Activation(relu))\n",
      "  (dense2): Dense(None -> 64, linear)\n",
      ")\n",
      "\n",
      "[[ 0.0094601   0.01150595  0.0043      0.01612011  0.00722826  0.          0.\n",
      "   0.          0.00676135  0.          0.          0.00687655  0.          0.\n",
      "   0.02170436  0.00075789  0.00781166  0.          0.          0.\n",
      "   0.01066998  0.01752731  0.03945627  0.          0.00320272  0.\n",
      "   0.00817525  0.          0.          0.          0.          0.00442885\n",
      "   0.01928487  0.01963375  0.          0.          0.          0.00038812\n",
      "   0.01876653  0.01475956  0.00426263  0.0241504   0.01523386  0.00143459\n",
      "   0.          0.02752425  0.00350364  0.          0.01664411  0.00395365\n",
      "   0.00881247  0.02001468  0.00148547  0.00085829  0.          0.00322197\n",
      "   0.          0.          0.00403514  0.          0.00884278  0.          0.\n",
      "   0.00168418]\n",
      " [ 0.          0.00558753  0.00849366  0.0035863   0.          0.\n",
      "   0.00692048  0.01070072  0.          0.          0.          0.00113965\n",
      "   0.          0.          0.00552788  0.01083942  0.01779256  0.\n",
      "   0.00397848  0.          0.01505047  0.0192869   0.03373974  0.\n",
      "   0.00518067  0.          0.01699411  0.          0.          0.          0.\n",
      "   0.00766116  0.          0.          0.          0.          0.00502465\n",
      "   0.00105198  0.00065004  0.00648416  0.00931723  0.02642786  0.0075257\n",
      "   0.          0.          0.01022802  0.00694586  0.          0.          0.\n",
      "   0.01258702  0.02297065  0.          0.01335956  0.          0.001127\n",
      "   0.00268761  0.00099226  0.00110631  0.          0.01392341  0.          0.\n",
      "   0.00577043]\n",
      " [ 0.01837424  0.00452966  0.02134358  0.0194915   0.01150653  0.00180834\n",
      "   0.          0.00554234  0.          0.          0.          0.01114757\n",
      "   0.          0.01136998  0.00602578  0.01367646  0.01123678  0.01382062\n",
      "   0.          0.          0.00228396  0.00665153  0.04862887  0.          0.006437\n",
      "   0.          0.00656061  0.          0.          0.          0.\n",
      "   0.00165103  0.02212063  0.0339928   0.          0.          0.          0.\n",
      "   0.0273196   0.01778429  0.          0.01819945  0.          0.0084863\n",
      "   0.          0.01105535  0.00286012  0.          0.01997411  0.00229498\n",
      "   0.01311682  0.0073318   0.          0.00461082  0.          0.00666856\n",
      "   0.          0.          0.01268346  0.          0.          0.00042983\n",
      "   0.01224728  0.00171525]\n",
      " [ 0.01461463  0.01804813  0.          0.01886739  0.00640675  0.          0.\n",
      "   0.01070938  0.          0.          0.          0.01180928  0.\n",
      "   0.00194217  0.00442072  0.02083457  0.00315258  0.00309546  0.\n",
      "   0.00226321  0.00135281  0.01249096  0.03818391  0.          0.00653459\n",
      "   0.          0.00995214  0.          0.          0.          0.\n",
      "   0.00023824  0.03186312  0.00504526  0.          0.          0.          0.\n",
      "   0.01383868  0.01833289  0.00565818  0.01444081  0.0015872   0.          0.\n",
      "   0.02690128  0.0099966   0.          0.00498926  0.          0.00254092\n",
      "   0.00267474  0.00099182  0.          0.          0.00320558  0.00142753\n",
      "   0.          0.00601305  0.          0.01004975  0.01057182  0.0069926\n",
      "   0.        ]]\n",
      "<NDArray 4x64 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "class RecMLP1(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RecMLP1, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.dense0 = nn.Dense(256, activation=\"relu\")\n",
    "            self.dense1 = nn.Dense(128, activation=\"relu\")\n",
    "            self.dense2 = nn.Dense(64)\n",
    "#             self.denses = [nn.Dense(256, activation=\"relu\"), nn.Dense(128, activation=\"relu\"), nn.Dense(64) ]\n",
    "            # __init__,forward,  函数输入参数要求Block类， self.denses为type list\n",
    "\n",
    "    def forward(self, x):\n",
    "#         for dense in self.denses:\n",
    "#             x = dense(x)\n",
    "        x = self.dense2(self.dense1(self.dense0(x)))\n",
    "        return nd.relu(x)\n",
    "\n",
    "rec_mlp1 = RecMLP1()\n",
    "rec_mlp1.initialize()\n",
    "print(rec_mlp1)\n",
    "print(rec_mlp1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(4, activation=\"relu\"))\n",
    "        net.add(nn.Dense(2))\n",
    "    return net\n",
    "\n",
    "x = nd.random.uniform(shape=(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter sequential13_dense0_bias has not been initialized. Note that you should initialize parameters and create Trainer with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    net = get_net()\n",
    "    net(x)\n",
    "except RuntimeError as err:\n",
    "    sys.stderr.write(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.00058652  0.00016143]\n",
       " [-0.00042701  0.00025122]\n",
       " [ 0.00015206  0.00101848]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问模型参数\n",
    "之前我们提到过可以通过`weight`和`bias`访问`Dense`的参数，他们是`Parameter`这个类。\n",
    "\n",
    "然后我们可以通过`data`来访问参数，`grad`来访问对应的梯度\n",
    "\n",
    "我们也可以通过<font color=red> collect_params</font>来访问`Block`里面所有的参数（这个会包括所有的子`Block`）。它会返回一个名字到对应`Parameter`的`dict`。既可以用正常[]来访问参数，也可以用`get()`，它不需要填写名字的前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  sequential13_dense0 \n",
      "weight:  Parameter sequential13_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>) \n",
      "bias:  Parameter sequential13_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "weight data:  \n",
      "[[ 0.04835007 -0.01382367  0.00507843  0.0601008   0.02523782]\n",
      " [-0.05605391  0.01528487  0.06234222 -0.05621308  0.0517284 ]\n",
      " [-0.05711614 -0.00641727 -0.06216478 -0.02426187 -0.05788545]\n",
      " [-0.03741582 -0.03679574  0.01602506  0.04753181 -0.06536956]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "weight grad:  \n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "<NDArray 4x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight\n",
    "b = net[0].bias\n",
    "print('name: ', net[0].name, '\\nweight: ', w, '\\nbias: ', b)\n",
    "print('weight data: ', w.data(), '\\nweight grad: ', w.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential13_ (\n",
      "  Parameter sequential13_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential13_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential13_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential13_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[[ 0.04835007 -0.01382367  0.00507843  0.0601008   0.02523782]\n",
      " [-0.05605391  0.01528487  0.06234222 -0.05621308  0.0517284 ]\n",
      " [-0.05711614 -0.00641727 -0.06216478 -0.02426187 -0.05788545]\n",
      " [-0.03741582 -0.03679574  0.01602506  0.04753181 -0.06536956]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "\n",
      "[ 0.  0.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "print(params)\n",
    "print(params['sequential13_dense0_bias'].data())\n",
    "print(params.get('dense0_weight').data())\n",
    "print(params.get('dense1_bias').data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用不同的初始函数来初始化\n",
    "我们一直在使用默认的initialize来初始化权重（除了指定GPU ctx外）。它会把所有权重初始化成在[-0.07, 0.07]之间均匀分布的随机数。我们可以使用别的初始化方法。例如使用均值为0，方差为0.02的正态分布\n",
    "## 共享模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.01000567  0.00703028 -0.0004781   0.00348184 -0.01082114]\n",
      " [-0.00994558  0.02120297  0.02790563 -0.00359711 -0.01372356]\n",
      " [ 0.01808388  0.02528431 -0.0129267   0.02320206  0.01585069]\n",
      " [ 0.02222755 -0.00252373 -0.01147721  0.01738829  0.03108984]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import init\n",
    "params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享模型参数\n",
    "有时候我们想在层之间共享同一份参数，我们可以通过Block的params输出参数来手动指定参数，而不是让系统自动生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.0514059  -0.01693203 -0.01760576 -0.01759853]\n",
      " [-0.02458332  0.03483035  0.02521617 -0.03670699]\n",
      " [ 0.04137486 -0.04594057  0.0005507  -0.00709917]\n",
      " [-0.02852607 -0.02737442  0.05403472  0.04748648]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[-0.0514059  -0.01693203 -0.01760576 -0.01759853]\n",
      " [-0.02458332  0.03483035  0.02521617 -0.03670699]\n",
      " [ 0.04137486 -0.04594057  0.0005507  -0.00709917]\n",
      " [-0.02852607 -0.02737442  0.05403472  0.04748648]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, activation=\"relu\", params=net[-1].params))\n",
    "    net.add(nn.Dense(2))\n",
    "    \n",
    "net.initialize()\n",
    "net(x)\n",
    "print(net[1].weight.data())\n",
    "print(net[2].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  自定义初始化方法\n",
    "下面我们自定义一个初始化方法。它通过重载_init_weight来实现不同的初始化方法。（注意到Gluon里面bias都是默认初始化成0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n",
      "init weight (4, 5)\n",
      "init weight (2, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 7.5135479   7.02971172  5.76813793  5.12156582  7.1275382 ]\n",
       " [ 6.71305466  9.53085899  8.11115551  5.04255772  6.39533997]\n",
       " [ 9.8443203   6.04874992  8.45446873  5.57851601  5.49708176]\n",
       " [ 7.88570118  6.44887924  8.47634983  7.69536781  8.35978508]]\n",
       "<NDArray 4x5 @cpu(0)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        # 初始化权重，使用out=arr后我们不需指定形状\n",
    "        print('init weight', arr.shape)\n",
    "        nd.random.uniform(low=5, high=10, out=arr)\n",
    "        \n",
    "net = get_net()\n",
    "net.initialize(MyInit())\n",
    "print(x.shape)\n",
    "net(x)\n",
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function uniform in module mxnet.ndarray.random:\n",
      "\n",
      "uniform(low=0, high=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs)\n",
      "    Draw random samples from a uniform distribution.\n",
      "    \n",
      "    Samples are uniformly distributed over the half-open interval *[low, high)*\n",
      "    (includes *low*, but excludes *high*).\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    low : float or NDArray\n",
      "        Lower boundary of the output interval. All values generated will be\n",
      "        greater than or equal to low. The default value is 0.\n",
      "    high : float or NDArray\n",
      "        Upper boundary of the output interval. All values generated will be\n",
      "        less than high. The default value is 1.0.\n",
      "    shape : int or tuple of ints\n",
      "        The number of samples to draw. If shape is, e.g., `(m, n)` and `low` and\n",
      "        `high` are scalars, output shape will be `(m, n)`. If `low` and `high`\n",
      "        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n",
      "        `(x, y, m, n)`, where `m*n` samples are drawn for each `[low, high)` pair.\n",
      "    dtype : {'float16','float32', 'float64'}\n",
      "        Data type of output samples. Default is 'float32'\n",
      "    ctx : Context\n",
      "        Device context of output. Default is current context. Overridden by\n",
      "        `low.context` when `low` is an NDArray.\n",
      "    out : NDArray\n",
      "        Store output to an existing NDArray.\n",
      "    \n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> mx.nd.random.uniform(0, 1)\n",
      "    [ 0.54881352]\n",
      "    <NDArray 1 @cpu(0)\n",
      "    >>>> mx.nd.random.uniform(0, 1, ctx=mx.gpu(0))\n",
      "    [ 0.92514056]\n",
      "    <NDArray 1 @gpu(0)>\n",
      "    >>> mx.nd.random.uniform(-1, 1, shape=(2,))\n",
      "    [[ 0.71589124  0.08976638]\n",
      "     [ 0.69450343 -0.15269041]]\n",
      "    <NDArray 2x2 @cpu(0)>\n",
      "    >>> low = mx.nd.array([1,2,3])\n",
      "    >>> high = mx.nd.array([2,3,4])\n",
      "    >>> mx.nd.random.uniform(low, high, shape=2)\n",
      "    [[ 1.78653979  1.93707538]\n",
      "     [ 2.01311183  2.37081361]\n",
      "     [ 3.30491424  3.69977832]]\n",
      "    <NDArray 3x2 @cpu(0)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nd.random.uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然我们也可以通过Parameter.set_data来直接改写权重。注意到由于有延后初始化，所以我们通常可以通过调用一次net(x)来确定权重的形状先"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default weight: \n",
      "[[ 0.06581051 -0.0137601  -0.06643037 -0.03522212]\n",
      " [ 0.05432612  0.0008213   0.02920524 -0.02654669]]\n",
      "<NDArray 2x4 @cpu(0)>\n",
      "init to all 1s: \n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "net.initialize()\n",
    "net(x)\n",
    "\n",
    "print('default weight:', net[1].weight.data())\n",
    "\n",
    "w = net[1].weight\n",
    "w.set_data(nd.ones(w.shape))\n",
    "\n",
    "print('init to all 1s:', net[1].weight.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential19_ (\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " |  params\n",
    " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
    " |      children's parameters)\n",
    "\"\"\"\n",
    "net0 = get_net()\n",
    "net.add(net0)\n",
    "# params = net.collect_params()\n",
    "# params.initialize(init=init.Uniform(),force_reinit= True)\n",
    "net.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method initialize in module mxnet.gluon.block:\n",
      "\n",
      "initialize(init=<mxnet.initializer.Uniform object at 0x00000000046946A0>, ctx=None, verbose=False) method of mxnet.gluon.nn.basic_layers.Sequential instance\n",
      "    Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      "    \n",
      "    Equivalent to ``block.collect_params().initialize(...)``\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(net.initialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Block.collect_params of Sequential(\n",
       "  (0): Dense(5 -> 4, Activation(relu))\n",
       "  (1): Dense(4 -> 2, linear)\n",
       ")>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " |  collect_params(self)\n",
    " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
    " |      children's Parameters.\n",
    "\"\"\"\n",
    "net.collect_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 8.62073898  9.74430466  6.94433689  5.01351595]\n",
      " [ 6.13541985  8.23598289  7.27429962  8.00196075]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sequential19'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net.params('dense1_weight').data()\n",
    "print(net.collect_params().get('dense1_weight').data())\n",
    "net.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Sequential in module mxnet.gluon.nn.basic_layers object:\n",
      "\n",
      "class Sequential(mxnet.gluon.block.Block)\n",
      " |  Stacks Blocks sequentially.\n",
      " |  \n",
      " |  Example::\n",
      " |  \n",
      " |      net = nn.Sequential()\n",
      " |      # use net's name_scope to give child Blocks appropriate names.\n",
      " |      with net.name_scope():\n",
      " |          net.add(nn.Dense(10, activation='relu'))\n",
      " |          net.add(nn.Dense(20))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __init__(self, prefix=None, params=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, *blocks)\n",
      " |      Adds block on top of the stack.\n",
      " |  \n",
      " |  forward(self, x)\n",
      " |      Overrides to implement forward computation using :py:class:`NDArray`. Only\n",
      " |      accepts positional arguments.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *args : list of NDArray\n",
      " |          Input tensors.\n",
      " |  \n",
      " |  hybridize(self, active=True)\n",
      " |      Activates or deactivates `HybridBlock`s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast this Block to use another data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  collect_params(self)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters.\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x00000000046946A0>, ctx=None, verbose=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      \n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |  \n",
      " |  load_params(self, filename, ctx, allow_missing=False, ignore_extra=False)\n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context\n",
      " |          Context(s) initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |  \n",
      " |  register_child(self, block)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
