{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 多类逻辑回归-gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 获取和读取数据\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "batch_size = 256\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. 定义和初始化模型\n",
    "利用Flatten层将输入数据变换成batch_size X ? 的大小，在输入到输出大小为10的全连接层，不要指定中间层的大小，gluon会自动推导\n",
    "\"\"\"\n",
    "from mxnet import gluon\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. softmax和交叉熵损失\n",
    "在softmax regression 从0开始一节中，我们增大学习率时，会导致反向求导过程中数据溢出，\n",
    "主要原因是分开定义Softmax和交叉熵这样会有数值不稳定性，在分开计算过程中增加计算次数，在浮点数运算过程中，会有累积误差产生。\n",
    "因此gluon提供一个将这两个函数合起来的数值更稳定的版本\n",
    "\"\"\"\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "softmax 与 cross entropy 分开与结合版\n",
    "\"\"\"\n",
    "# function softmax(z)\n",
    "#   #z = z - maximum(z)\n",
    "#   o = exp(z)\n",
    "#   return o / sum(o)\n",
    "# end\n",
    "# function gradient_together(z, y)\n",
    "#   o = softmax(z)\n",
    "#   o[y] -= 1.0\n",
    "#   return o\n",
    "# end\n",
    "# function gradient_separated(z, y)\n",
    "#   o = softmax(z)\n",
    "#   ∂o_∂z = diagm(o) - o*o'\n",
    "#   ∂f_∂o = zeros(size(o))\n",
    "#   ∂f_∂o[y] = -1.0 / o[y]\n",
    "#   return ∂o_∂z * ∂f_∂o\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SoftmaxCrossEntropyLoss in module mxnet.gluon.loss object:\n",
      "\n",
      "class SoftmaxCrossEntropyLoss(Loss)\n",
      " |  Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)\n",
      " |  \n",
      " |  If `sparse_label` is `True` (default), label should contain integer\n",
      " |  category indicators:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      \\DeclareMathOperator{softmax}{softmax}\n",
      " |  \n",
      " |      p = \\softmax({pred})\n",
      " |  \n",
      " |      L = -\\sum_i \\log p_{i,{label}_i}\n",
      " |  \n",
      " |  `label`'s shape should be `pred`'s shape with the `axis` dimension removed.\n",
      " |  i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape should\n",
      " |  be (1,2,4).\n",
      " |  \n",
      " |  If `sparse_label` is `False`, `label` should contain probability distribution\n",
      " |  and `label`'s shape should be the same with `pred`:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      p = \\softmax({pred})\n",
      " |  \n",
      " |      L = -\\sum_i \\sum_j {label}_j \\log p_{ij}\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  axis : int, default -1\n",
      " |      The axis to sum over when computing softmax and entropy.\n",
      " |  sparse_label : bool, default True\n",
      " |      Whether label is an integer array instead of probability distribution.\n",
      " |  from_logits : bool, default False\n",
      " |      Whether input is a log probability (usually from log_softmax) instead\n",
      " |      of unnormalized numbers.\n",
      " |  weight : float or None\n",
      " |      Global scalar weight for loss.\n",
      " |  batch_axis : int, default 0\n",
      " |      The axis that represents mini-batch.\n",
      " |  \n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **pred**: the prediction tensor, where the `batch_axis` dimension\n",
      " |        ranges over batch size and `axis` dimension ranges over the number\n",
      " |        of classes.\n",
      " |      - **label**: the truth tensor. When `sparse_label` is True, `label`'s\n",
      " |        shape should be `pred`'s shape with the `axis` dimension removed.\n",
      " |        i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape\n",
      " |        should be (1,2,4) and values should be integers between 0 and 2. If\n",
      " |        `sparse_label` is False, `label`'s shape must be the same as `pred`\n",
      " |        and values should be floats in the range `[0, 1]`.\n",
      " |      - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n",
      " |        to the same shape as label. For example, if label has shape (64, 10)\n",
      " |        and you want to weigh each sample in the batch separately,\n",
      " |        sample_weight should have shape (64, 1).\n",
      " |  \n",
      " |  Outputs:\n",
      " |      - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n",
      " |        batch_axis are averaged out.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SoftmaxCrossEntropyLoss\n",
      " |      Loss\n",
      " |      mxnet.gluon.block.HybridBlock\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, axis=-1, sparse_label=True, from_logits=False, weight=None, batch_axis=0, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  hybrid_forward(self, F, pred, label, sample_weight=None)\n",
      " |      Overrides to construct symbolic graph for this `Block`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Symbol or NDArray\n",
      " |          The first input tensor.\n",
      " |      *args : list of Symbol or list of NDArray\n",
      " |          Additional input tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Loss:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.HybridBlock:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast this Block to use another data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  export(self, path)\n",
      " |      Export HybridBlock to json format that can be loaded by `mxnet.mod.Module`\n",
      " |      or the C++ interface.\n",
      " |      \n",
      " |      .. note:: When there are only one input, it will have name `data`. When there\n",
      " |                Are more than one inputs, they will be named as `data0`, `data1`, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Path to save model. Two files `path-symbol.json` and `path-0000.params`\n",
      " |          will be created.\n",
      " |  \n",
      " |  forward(self, x, *args)\n",
      " |      Defines the forward computation. Arguments can be either\n",
      " |      :py:class:`NDArray` or :py:class:`Symbol`.\n",
      " |  \n",
      " |  hybridize(self, active=True)\n",
      " |      Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |  \n",
      " |  infer_shape(self, *args)\n",
      " |      Infers shape of Parameters from inputs.\n",
      " |  \n",
      " |  infer_type(self, *args)\n",
      " |      Infers data type of Parameters from inputs.\n",
      " |  \n",
      " |  register_child(self, block)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  collect_params(self)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters.\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x000000000467F5F8>, ctx=None, verbose=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      \n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |  \n",
      " |  load_params(self, filename, ctx, allow_missing=False, ignore_extra=False)\n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context\n",
      " |          Context(s) initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. 优化\n",
    "\"\"\"\n",
    "trainer = gluon.Trainer(net.collect_params(),optimizer='sgd',optimizer_params={'learning_rate': 10.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss:36.672965, Train accuracy: 0.684395, Test accuracy: 0.659255.\n",
      "Epoch 1. Loss:20.201806, Train accuracy: 0.755142, Test accuracy: 0.786859.\n",
      "Epoch 2. Loss:17.588913, Train accuracy: 0.771267, Test accuracy: 0.782953.\n",
      "Epoch 3. Loss:16.779442, Train accuracy: 0.778112, Test accuracy: 0.744491.\n",
      "Epoch 4. Loss:16.465251, Train accuracy: 0.781267, Test accuracy: 0.701623.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5. 训练\n",
    "\"\"\"\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import autograd\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_acc = 0.\n",
    "    train_loss = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_acc += utils.accuracy(output, label)\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        \n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print('Epoch %d. Loss:%f, Train accuracy: %f, Test accuracy: %f.'%(epoch, train_loss/len(train_data),\n",
    "                                                                      train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
