{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 读取数据集\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "batch_size = 256\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数初始化\n",
    "我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。\n",
    "\n",
    "**错误：全零初始化**。让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。\n",
    "\n",
    "**小随机数初始化**。因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：W = 0.01 * np.random.randn(D,H)。其中randn函数是基于零均值和标准差的一个高斯分布（译者注：国内教程一般习惯称均值参数为期望\\mu）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。\n",
    "\n",
    "*警告*。并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。\n",
    "\n",
    "使用$1/sqrt(n)$校准方差。上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：w = np.random.randn(n) / sqrt(n)。其中n是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。\n",
    "\n",
    "上述结论的推导过程如下：假设权重w和输入x之间的内积为$s=\\sum^n_iw_ix_i$，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查s的方差：\n",
    "\n",
    "$$\\displaystyle Var(s)=Var(\\sum^n_iw_ix_i) $$\n",
    "$$\\displaystyle =\\sum^n_iVar(w_ix_i) $$\n",
    "$$\\displaystyle =\\sum^n_i[E(w_i)]^2Var(x_i)+E[(x_i)]^2Var(w_i)+Var(xIi)Var(w_i)$$\n",
    "$$\\displaystyle =\\sum^n_iVar(x_i)Var(w_i)$$\n",
    "$$\\displaystyle =(nVar(w))Var(x) $$\n",
    "在前两步，使用了方差的性质。在第三步，因为假设输入和权重的平均值都是0，所以$E[x_i]=E[w_i]=0$。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的$w_i,x_i$都服从同样的分布。从这个推导过程我们可以看见，如果想要s有和输入x一样的方差，那么在初始化的时候必须保证每个权重w的方差是1/n。又因为对于一个随机变量X和标量a，有$Var(aX)=a^2Var(X)$，这就说明可以基于一个标准高斯分布，然后乘以$a=\\sqrt{1/n}$，使其方差为1/n，于是得出：w = np.random.randn(n) / sqrt(n)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. 多层感知机\n",
    "多层感知机与多类的逻辑回归非常类似，区别在于多层感知机的输入与输出层之间加入了一个或多个隐含层\n",
    "\"\"\"\n",
    "from mxnet import ndarray as nd\n",
    "import numpy as np\n",
    "num_inputs = 28 * 28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden1 = 256\n",
    "weight_scale = .01\n",
    "\n",
    "num_hidden2 = 512\n",
    "\n",
    "w1 = nd.random_normal(shape=(num_inputs, num_hidden1),scale = weight_scale)\n",
    "b1 = nd.zeros(num_hidden1)\n",
    "\n",
    "w2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale= weight_scale)\n",
    "b2 = nd.zeros(num_hidden2)\n",
    "\n",
    "w3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale= weight_scale)\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "\n",
    "params = [w1, b1, w2, b2, w3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加隐藏层使得收敛变快，并没有明显的提高模型精度，初始化权重对模型训练精度影响很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. 激活函数\n",
    "激活函数选择非线性函数，因为多层线性函数的乘积仍然是线性函数\n",
    "\"\"\"\n",
    "def relu(X):\n",
    "    return nd.maximum(X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. 定义模型\n",
    "\"\"\"\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    h1 = relu(nd.dot(X, w1) + b1)\n",
    "    h2 = relu(nd.dot(h1, w2) + b2)\n",
    "    output = nd.dot(h2, w3) + b3\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. softmax和交叉熵损失函数\n",
    "\"\"\"\n",
    "from mxnet import gluon\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss: 1.081886, Train acc:0.579711, Test acc:0.779447\n",
      "Epoch:1, Loss: 0.538025, Train acc:0.798644, Test acc:0.833934\n",
      "Epoch:2, Loss: 0.456091, Train acc:0.831180, Test acc:0.849359\n",
      "Epoch:3, Loss: 0.421083, Train acc:0.844518, Test acc:0.852865\n",
      "Epoch:4, Loss: 0.383077, Train acc:0.857772, Test acc:0.847857\n",
      "Epoch:5, Loss: 0.363218, Train acc:0.866553, Test acc:0.853466\n",
      "Epoch:6, Loss: 0.345206, Train acc:0.871895, Test acc:0.855469\n",
      "Epoch:7, Loss: 0.337892, Train acc:0.874533, Test acc:0.873698\n",
      "Epoch:8, Loss: 0.321238, Train acc:0.880843, Test acc:0.870593\n",
      "Epoch:9, Loss: 0.309073, Train acc:0.884348, Test acc:0.879607\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6. 训练\n",
    "\"\"\"\n",
    "from mxnet import autograd\n",
    "\n",
    "learning_rate = .5\n",
    "for epoch in range(10):\n",
    "    train_acc = 0.\n",
    "    train_loss = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        utils.SGD(params, learning_rate/batch_size)\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += utils.accuracy(output, label)\n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print('Epoch:%d, Loss: %f, Train acc:%f, Test acc:%f'%(epoch, train_loss/len(train_data),\n",
    "                                                           train_acc/len(train_data),test_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.sqrt(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
