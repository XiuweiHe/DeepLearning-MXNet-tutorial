{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃法的概念\n",
    "在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：\n",
    "\n",
    "* 随机选择一部分该层的输出作为丢弃元素；\n",
    "* 把丢弃元素乘以0；\n",
    "* 把非丢弃元素拉伸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃法的实现\n",
    "丢弃法的实现很容易，例如像下面这样。这里的标量<font color=red > drop_probability </font>定义了一个<font color=red> X（NDArray类）</font>中任何一个元素被丢弃的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "\n",
    "def dropout(X, drop_probability):\n",
    "    keep_probability = 1 - drop_probability\n",
    "    assert 0 <= keep_probability <= 1\n",
    "    if keep_probability == 0:\n",
    "        return X.zero_like()\n",
    "    mask = nd.random.uniform(0, 1.0, X.shape, ctx = X.context) < keep_probability\n",
    "    # 保证 E[dropout(X)] == X 期望不变\n",
    "    scale = 1 / keep_probability\n",
    "    return mask * X * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  0.   1.   2.   3.]\n",
       " [  4.   5.   6.   7.]\n",
       " [  8.   9.  10.  11.]\n",
       " [ 12.  13.  14.  15.]\n",
       " [ 16.  17.  18.  19.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.arange(20).reshape((5,4))\n",
    "dropout(A, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  0.   0.   4.   6.]\n",
       " [  8.   0.  12.   0.]\n",
       " [ 16.  18.  20.  22.]\n",
       " [  0.   0.   0.   0.]\n",
       " [ 32.   0.   0.   0.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(A, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃法的本质\n",
    "了解了丢弃法的概念与实现，那你可能对它的本质产生了好奇。\n",
    "\n",
    "如果你了解集成学习，你可能知道它在提升弱分类器准确率上的威力。一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。\n",
    "\n",
    "事实上，丢弃法在模拟集成学习。试想，一个使用了丢弃法的多层神经网络本质上是原始网络的子集（节点和边）.\n",
    "我们在之前的章节里介绍过随机梯度下降算法：我们在训练神经网络模型时一般随机采样一个批量的训练数据。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。与一般的集成学习不同，这里<font color=red>每个原神经网络子集的分类器用的是同一套参数</font>。因此丢弃法只是在模拟集成学习。\n",
    "\n",
    "我们刚刚强调了，原神经网络子集的分类器在不同的训练数据批量上训练并使用同一套参数。因此，<font color =red>使用丢弃法的神经网络实质上是对输入层和隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好</font>。\n",
    "\n",
    "下面我们动手实现一下在多层神经网络里加丢弃层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 数据获取\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "batch_size = 256\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. 定义多层感知机\n",
    "\"\"\"\n",
    "num_inputs = 28 * 28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden1 = 256\n",
    "num_hidden2 = 256\n",
    "weight_scale = .01\n",
    "\n",
    "w1 = nd.random_normal(shape=(num_inputs, num_hidden1),scale= weight_scale)\n",
    "b1 = nd.zeros(num_hidden1)\n",
    "\n",
    "w2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale= weight_scale)\n",
    "b2 = nd.zeros(num_hidden2)\n",
    "\n",
    "w3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale= weight_scale)\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [w1, b1, w2, b2, w3,b3]\n",
    "for para in params:\n",
    "    para.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.定义包含丢弃层的模型\n",
    "我们的模型就是将层（全连接）和激活函数（Relu）串起来，并在应用激活函数后添加丢弃层。每个丢弃层的元素丢弃概率可以分别设置。\n",
    "一般情况下，我们推荐把更靠近输入层的元素丢弃概率设的更小一点。这个试验中，我们把第一层全连接后的元素丢弃概率设为0.2，\n",
    "把第二层全连接后的元素丢弃概率设为0.5\n",
    "\"\"\"\n",
    "drop_prob1 = 0.4\n",
    "drop_prob2 = 0.8\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    # first fully connected layer\n",
    "    h1 = nd.relu(nd.dot(X, w1) + b1)\n",
    "    # add dropout layer to the first layer behind\n",
    "    if autograd.is_training:\n",
    "        h1 = dropout(h1, drop_prob1)\n",
    "    # second fully connected layer\n",
    "    h2 = nd.relu(nd.dot(h1, w2) + b2)\n",
    "    # add dropout layer to the second layer behind\n",
    "    if autograd.is_training:\n",
    "        h2 = dropout(h2, drop_prob2)\n",
    "    return nd.dot(h2, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.233198, Train acc 0.915398, Test acc 0.875100\n",
      "Epoch 1. Loss: 0.237649, Train acc 0.913929, Test acc 0.863381\n",
      "Epoch 2. Loss: 0.237582, Train acc 0.914830, Test acc 0.875100\n",
      "Epoch 3. Loss: 0.228922, Train acc 0.916249, Test acc 0.872196\n",
      "Epoch 4. Loss: 0.231357, Train acc 0.916116, Test acc 0.871895\n",
      "Epoch 5. Loss: 0.229443, Train acc 0.916950, Test acc 0.872095\n",
      "Epoch 6. Loss: 0.235752, Train acc 0.915281, Test acc 0.875100\n",
      "Epoch 7. Loss: 0.230363, Train acc 0.916283, Test acc 0.874900\n",
      "Epoch 8. Loss: 0.230902, Train acc 0.919788, Test acc 0.866987\n",
      "Epoch 9. Loss: 0.230377, Train acc 0.917134, Test acc 0.878005\n",
      "Epoch 10. Loss: 0.233277, Train acc 0.916834, Test acc 0.878005\n",
      "Epoch 11. Loss: 0.231997, Train acc 0.917768, Test acc 0.874099\n",
      "Epoch 12. Loss: 0.227141, Train acc 0.918787, Test acc 0.875401\n",
      "Epoch 13. Loss: 0.227402, Train acc 0.918069, Test acc 0.874800\n",
      "Epoch 14. Loss: 0.225577, Train acc 0.918286, Test acc 0.873097\n",
      "Epoch 15. Loss: 0.230774, Train acc 0.917551, Test acc 0.874800\n",
      "Epoch 16. Loss: 0.230112, Train acc 0.917435, Test acc 0.874700\n",
      "Epoch 17. Loss: 0.227656, Train acc 0.918520, Test acc 0.868089\n",
      "Epoch 18. Loss: 0.226272, Train acc 0.919722, Test acc 0.872796\n",
      "Epoch 19. Loss: 0.223538, Train acc 0.920005, Test acc 0.874700\n",
      "Epoch 20. Loss: 0.226853, Train acc 0.918403, Test acc 0.875300\n",
      "Epoch 21. Loss: 0.229575, Train acc 0.918887, Test acc 0.867989\n",
      "Epoch 22. Loss: 0.223346, Train acc 0.920089, Test acc 0.875501\n",
      "Epoch 23. Loss: 0.219913, Train acc 0.919905, Test acc 0.874499\n",
      "Epoch 24. Loss: 0.217295, Train acc 0.921424, Test acc 0.871695\n",
      "Epoch 25. Loss: 0.221044, Train acc 0.920857, Test acc 0.871595\n",
      "Epoch 26. Loss: 0.218740, Train acc 0.922359, Test acc 0.875300\n",
      "Epoch 27. Loss: 0.219158, Train acc 0.921958, Test acc 0.876102\n",
      "Epoch 28. Loss: 0.221948, Train acc 0.919438, Test acc 0.878906\n",
      "Epoch 29. Loss: 0.220792, Train acc 0.920489, Test acc 0.868089\n",
      "Epoch 30. Loss: 0.220642, Train acc 0.920957, Test acc 0.877103\n",
      "Epoch 31. Loss: 0.218783, Train acc 0.921391, Test acc 0.874599\n",
      "Epoch 32. Loss: 0.221184, Train acc 0.921140, Test acc 0.875401\n",
      "Epoch 33. Loss: 0.223286, Train acc 0.920723, Test acc 0.880409\n",
      "Epoch 34. Loss: 0.222273, Train acc 0.919488, Test acc 0.878405\n",
      "Epoch 35. Loss: 0.216974, Train acc 0.921725, Test acc 0.871194\n",
      "Epoch 36. Loss: 0.215488, Train acc 0.923094, Test acc 0.875601\n",
      "Epoch 37. Loss: 0.220181, Train acc 0.922443, Test acc 0.875000\n",
      "Epoch 38. Loss: 0.217629, Train acc 0.921040, Test acc 0.878205\n",
      "Epoch 39. Loss: 0.220268, Train acc 0.920823, Test acc 0.872796\n",
      "Epoch 40. Loss: 0.215426, Train acc 0.921224, Test acc 0.873097\n",
      "Epoch 41. Loss: 0.216519, Train acc 0.923027, Test acc 0.875801\n",
      "Epoch 42. Loss: 0.215780, Train acc 0.922827, Test acc 0.877604\n",
      "Epoch 43. Loss: 0.216801, Train acc 0.921992, Test acc 0.877704\n",
      "Epoch 44. Loss: 0.216649, Train acc 0.921207, Test acc 0.870593\n",
      "Epoch 45. Loss: 0.216457, Train acc 0.920473, Test acc 0.871795\n",
      "Epoch 46. Loss: 0.215579, Train acc 0.922276, Test acc 0.876202\n",
      "Epoch 47. Loss: 0.217512, Train acc 0.921942, Test acc 0.871895\n",
      "Epoch 48. Loss: 0.215580, Train acc 0.922159, Test acc 0.875401\n",
      "Epoch 49. Loss: 0.210625, Train acc 0.922927, Test acc 0.868690\n",
      "Epoch cost time: 157.713144\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4. 训练\n",
    "\"\"\"\n",
    "from time import time\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "learning_rate = .5\n",
    "begin = time()\n",
    "for epoch in range(50):\n",
    "    train_acc = 0.\n",
    "    train_loss = 0.    \n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        utils.SGD(params, learning_rate/batch_size)\n",
    "        \n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += utils.accuracy(output, label)\n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data),\n",
    "        train_acc/len(train_data), test_acc))\n",
    "end = time()\n",
    "print(\"Epoch cost time: %f\"%(end - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未使用dropout 训练50轮的结果\n",
    "* 在第34轮之后训练出现异常，Loss变成Nan，train_acc 和 test_acc异常\n",
    "\n",
    "\n",
    "Epoch 26. Loss: 0.225771, Train acc 0.914413, Test acc 0.892127\n",
    "\n",
    "Epoch 27. Loss: 0.214575, Train acc 0.918570, Test acc 0.893930\n",
    "\n",
    "Epoch 28. Loss: 0.214624, Train acc 0.919655, Test acc 0.885116\n",
    "\n",
    "Epoch 29. Loss: 0.216177, Train acc 0.918636, Test acc 0.891426\n",
    "\n",
    "Epoch 30. Loss: 0.204961, Train acc 0.921875, Test acc 0.888321\n",
    "\n",
    "\n",
    "Epoch 31. Loss: 0.199285, Train acc 0.924663, Test acc 0.895733\n",
    "\n",
    "Epoch 32. Loss: 0.197781, Train acc 0.925047, Test acc 0.893229\n",
    "\n",
    "Epoch 33. Loss: 0.192677, Train acc 0.927367, Test acc 0.892728\n",
    "\n",
    "Epoch 34. Loss: 91424674556056240308355072.000000, Train acc 0.869541, Test acc 0.100060\n",
    "\n",
    "Epoch 35. Loss: nan, Train acc 0.100461, Test acc 0.099760\n",
    "\n",
    "Epoch 36. Loss: nan, Train acc 0.100077, Test acc 0.099760\n",
    "\n",
    "Epoch 37. Loss: nan, Train acc 0.099977, Test acc 0.099760\n",
    "\n",
    "Epoch 38. Loss: nan, Train acc 0.099960, Test acc 0.099760\n",
    "\n",
    "Epoch 48. Loss: nan, Train acc 0.100027, Test acc 0.099760\n",
    "\n",
    "Epoch 49. Loss: nan, Train acc 0.099993, Test acc 0.099760\n",
    "### 使用dropout后训练150轮的结果\n",
    "* 训练精度在持续提高，但测试精度基本稳定不变\n",
    "Epoch 30. Loss: 0.138990, Train acc 0.946698, Test acc 0.891226\n",
    "\n",
    "Epoch 36. Loss: 0.134528, Train acc 0.948584, Test acc 0.890224\n",
    "\n",
    "\n",
    "Epoch 41. Loss: 0.132773, Train acc 0.950087, Test acc 0.890625\n",
    "\n",
    "Epoch 46. Loss: 0.131675, Train acc 0.949770, Test acc 0.888922\n",
    "\n",
    "Epoch 47. Loss: 0.128813, Train acc 0.951372, Test acc 0.895032\n",
    "\n",
    "Epoch 48. Loss: 0.127396, Train acc 0.951973, Test acc 0.894431\n",
    "\n",
    "Epoch 49. Loss: 0.130470, Train acc 0.951038, Test acc 0.892127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
